{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential removal of links - QS 7 scenario\n",
    "\n",
    "- Disruption of a network by removal of links, based on:\n",
    "    + Sum of betweeness centrality of from and to nodes\n",
    "    + Link length\n",
    "    + Volume of commodity flow\n",
    "- Calculation of performance in terms of cost and unmet demand by re-running disrupted network on FOT\n",
    "- Comparison of multiple networks which vary in evenness\n",
    "- Plot link removal along x-axis and performance on y-axis, comparing networks of differing evenness\n",
    "\n",
    "To consider: Betweeness centrality of a subset of the graph, using only the edges used in a solution. Could use the facilities locations as the relevant nodes to do the subsetting.\n",
    "\n",
    "**Assumptions**\n",
    "\n",
    "- Working in a Python 3.x environment for this notebook\n",
    "- Python 2.7 installed as part of ArcGIS\n",
    "- 64 bit background geoprocessing enabled\n",
    "- Access to ArcGIS license server if necessary \n",
    "\n",
    "*Reference*\n",
    "\n",
    "- [NetworkX Documentation](https://networkx.github.io/documentation/stable/tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import sqlalchemy \n",
    "import networkx as nx\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "import resiliency_disruptions\n",
    "\n",
    "scen_name = 'qs7_rmp_proc_dest_multi_inputs'\n",
    "\n",
    "scen_path = os.path.join(\"C:\\\\FTOT\\\\scenarios\\\\quick_start\\\\\", scen_name)\n",
    "\n",
    "shp_path = os.path.join(scen_path, 'temp_networkx_shp_files')\n",
    "\n",
    "picklename = scen_name + 'BetweenessG.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(picklename):\n",
    "    file = open(picklename, 'rb')\n",
    "    betweenness_dict_road = pickle.load(file)\n",
    "    G_road = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by using betweeness centrality centrality calculation using networkX\n",
    "if not os.path.exists(picklename):\n",
    "    G_road = nx.read_shp(os.path.join(shp_path, 'road.shp'), simplify=True)\n",
    "    G_road = nx.convert_node_labels_to_integers(G_road, first_label=0, ordering='default', label_attribute=\"xy_coord_label\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run betweenness centrality on the NetworkX graph\n",
    "# !!! Slow !!!\n",
    "# This step might take 20+ minutes\n",
    "# Run if pickle not available\n",
    "if not os.path.exists(picklename):\n",
    "    print('Running Betweeness Centrality calculations. This might take more than 20 minutes.')\n",
    "    betweenness_dict_road = nx.betweenness_centrality(G_road, normalized=False, weight='MILES')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save with pickle\n",
    "# On load, need to know that there are two objects in this pickle, the betweeness centrality dict and the network G\n",
    "if not os.path.exists(picklename):\n",
    "    with open(picklename, 'wb') as handle:\n",
    "        pickle.dump(betweenness_dict_road, handle)\n",
    "        pickle.dump(G_road, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join Betweeness Centrality calculations to edges \n",
    "\n",
    "- Sum BC for each node of a link\n",
    "- Create data frame for repeated link removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\FTOT\\scenarios\\quick_start\\qs7_rmp_proc_dest_multi_inputs\n"
     ]
    }
   ],
   "source": [
    "# Read in FTOT data\n",
    "print(scen_path)\n",
    "db_name = 'main.db'\n",
    "\n",
    "db_path = 'sqlite:///' + os.path.join(scen_path, db_name)\n",
    "\n",
    "engine = sqlalchemy.create_engine(db_path)\n",
    "\n",
    "table_name = 'networkx_edges'\n",
    "nx_edges = pd.read_sql_table(table_name, engine)\n",
    "\n",
    "table_name = 'networkx_nodes'\n",
    "nx_nodes = pd.read_sql_table(table_name, engine)\n",
    "\n",
    "table_name = 'optimal_variables'\n",
    "optimal_vars = pd.read_sql_table(table_name, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_road_orig_label = nx.read_shp(os.path.join(shp_path, 'road.shp'), simplify=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "road_orig_label_nodes = list(G_road_orig_label.nodes) # these values are the shape_x and shape_y values in `networkx_nodes`. \n",
    "# Use that to get node_id from networkx_edges in the database,\n",
    "# Then use those id values to get edges info\n",
    "# Then line up the new integer labels with this list of ids to get betweeness centrality for each node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the betweeness_centrality values as the framework to join in shape_x, shape_y, and node_id\n",
    "bc_df_road = pd.DataFrame.from_dict(betweenness_dict_road, orient = 'index')\n",
    "bc_df_road = bc_df_road.rename(columns = {0: 'BC'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_shape_df_road = pd.DataFrame(road_orig_label_nodes)\n",
    "\n",
    "bc_shape_df_road = pd.concat([bc_df_road, node_shape_df_road], axis = 1)\n",
    "bc_shape_df_road = bc_shape_df_road.rename(columns = {0: 'shape_x', 1: 'shape_y'})\n",
    "\n",
    "# Now add node_id from networkx_nodes, using pandas merge with left join.\n",
    "# Use both shape_x and shape_y to identify the nodes correctly\n",
    "# Union of both prod and crude now\n",
    "\n",
    "bc_node_df = pd.merge(bc_shape_df_road, nx_nodes, on = ['shape_x', 'shape_y'], how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now use this data frame to populate a data frame of edges. \n",
    "# We will want the following from networkx_edges:\n",
    "# edge_id, from_node_id, to_node_id, mode_source, miles, mode_source_oid, \n",
    "# Then using the node_id column in the new bc_node_df, add these:\n",
    "# from_node_BC, to_node_BC\n",
    "# and sum those for sum_node_BC\n",
    "merge_from = pd.merge(nx_edges, bc_node_df[['BC','node_id']],\n",
    "                      left_on = 'from_node_id',\n",
    "                      right_on = 'node_id',\n",
    "                      how = 'left')\n",
    "merge_from = merge_from.rename(columns = {'BC': 'from_node_BC'})\n",
    "\n",
    "merge_to = pd.merge(merge_from, bc_node_df[['BC','node_id',]],\n",
    "                    left_on = 'to_node_id',\n",
    "                    right_on = 'node_id',\n",
    "                    how = 'left')\n",
    "merge_to = merge_to.rename(columns = {'BC': 'to_node_BC'})\n",
    "\n",
    "# Sum the BC values\n",
    "\n",
    "merge_to['sum_BC'] = merge_to.filter(like = \"node_BC\").sum(axis = 1)\n",
    "\n",
    "# Then from optimal_variables, get variable_name, nc_edge_id, mode, mode_oid, miles,\n",
    "# variable_value, converted_capacity, and converted_volume\n",
    "\n",
    "use_opt_vars = ['variable_type',\n",
    "               'var_id',\n",
    "               'variable_value',\n",
    "                'variable_name',\n",
    "                'nx_edge_id',\n",
    "                'mode_oid',\n",
    "                'converted_capacity',\n",
    "                'converted_volume'\n",
    "               ]\n",
    "\n",
    "merge_opt = pd.merge(merge_to, optimal_vars[use_opt_vars],\n",
    "                    left_on = 'edge_id',\n",
    "                    right_on = 'nx_edge_id',\n",
    "                    how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>edge_id</th>\n",
       "      <th>from_node_id</th>\n",
       "      <th>to_node_id</th>\n",
       "      <th>artificial</th>\n",
       "      <th>mode_source</th>\n",
       "      <th>mode_source_oid</th>\n",
       "      <th>miles</th>\n",
       "      <th>route_cost_scaling</th>\n",
       "      <th>capacity</th>\n",
       "      <th>volume</th>\n",
       "      <th>...</th>\n",
       "      <th>node_id_y</th>\n",
       "      <th>sum_BC</th>\n",
       "      <th>variable_type</th>\n",
       "      <th>var_id</th>\n",
       "      <th>variable_value</th>\n",
       "      <th>variable_name</th>\n",
       "      <th>nx_edge_id</th>\n",
       "      <th>mode_oid</th>\n",
       "      <th>converted_capacity</th>\n",
       "      <th>converted_volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>17378</td>\n",
       "      <td>2</td>\n",
       "      <td>pipeline_prod_trf_rts</td>\n",
       "      <td>3642</td>\n",
       "      <td>0.008758</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2555</td>\n",
       "      <td>0</td>\n",
       "      <td>rail</td>\n",
       "      <td>15397</td>\n",
       "      <td>0.090661</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1050.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>9614</td>\n",
       "      <td>0</td>\n",
       "      <td>rail</td>\n",
       "      <td>1054</td>\n",
       "      <td>0.148076</td>\n",
       "      <td>2.1</td>\n",
       "      <td>600.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   edge_id  from_node_id  to_node_id  artificial            mode_source  \\\n",
       "0        1             0       17378           2  pipeline_prod_trf_rts   \n",
       "1        2             1        2555           0                   rail   \n",
       "2        3             1        9614           0                   rail   \n",
       "\n",
       "   mode_source_oid     miles  route_cost_scaling  capacity  volume  ...  \\\n",
       "0             3642  0.008758                 1.0       NaN     NaN  ...   \n",
       "1            15397  0.090661                 1.6    1050.0     0.0  ...   \n",
       "2             1054  0.148076                 2.1     600.0     0.0  ...   \n",
       "\n",
       "   node_id_y  sum_BC  variable_type  var_id  variable_value  variable_name  \\\n",
       "0        NaN     0.0            NaN     NaN             NaN            NaN   \n",
       "1        NaN     0.0            NaN     NaN             NaN            NaN   \n",
       "2        NaN     0.0            NaN     NaN             NaN            NaN   \n",
       "\n",
       "  nx_edge_id  mode_oid  converted_capacity converted_volume  \n",
       "0        NaN       NaN                 NaN              NaN  \n",
       "1        NaN       NaN                 NaN              NaN  \n",
       "2        NaN       NaN                 NaN              NaN  \n",
       "\n",
       "[3 rows x 24 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_opt.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>edge_id</th>\n",
       "      <th>from_node_id</th>\n",
       "      <th>to_node_id</th>\n",
       "      <th>miles</th>\n",
       "      <th>capacity</th>\n",
       "      <th>volume</th>\n",
       "      <th>sum_BC</th>\n",
       "      <th>variable_type</th>\n",
       "      <th>variable_value</th>\n",
       "      <th>variable_name</th>\n",
       "      <th>nx_edge_id</th>\n",
       "      <th>mode_oid</th>\n",
       "      <th>converted_capacity</th>\n",
       "      <th>converted_volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>65011</td>\n",
       "      <td>65012</td>\n",
       "      <td>26325</td>\n",
       "      <td>16624</td>\n",
       "      <td>2.391027</td>\n",
       "      <td>45396.405255</td>\n",
       "      <td>44631.0</td>\n",
       "      <td>5276624.0</td>\n",
       "      <td>Edge</td>\n",
       "      <td>90.718474</td>\n",
       "      <td>Edge_173008</td>\n",
       "      <td>65012.0</td>\n",
       "      <td>12925.0</td>\n",
       "      <td>1.089514e+06</td>\n",
       "      <td>1071144.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>57386</td>\n",
       "      <td>57387</td>\n",
       "      <td>23196</td>\n",
       "      <td>183</td>\n",
       "      <td>3.527136</td>\n",
       "      <td>52814.692258</td>\n",
       "      <td>52543.0</td>\n",
       "      <td>4671108.0</td>\n",
       "      <td>Edge</td>\n",
       "      <td>90.718474</td>\n",
       "      <td>Edge_152421</td>\n",
       "      <td>57387.0</td>\n",
       "      <td>12949.0</td>\n",
       "      <td>1.267553e+06</td>\n",
       "      <td>1261032.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41214</td>\n",
       "      <td>41215</td>\n",
       "      <td>16624</td>\n",
       "      <td>7854</td>\n",
       "      <td>1.124715</td>\n",
       "      <td>50570.979389</td>\n",
       "      <td>46904.0</td>\n",
       "      <td>4404746.0</td>\n",
       "      <td>Edge</td>\n",
       "      <td>90.718474</td>\n",
       "      <td>Edge_109126</td>\n",
       "      <td>41215.0</td>\n",
       "      <td>12525.0</td>\n",
       "      <td>1.213704e+06</td>\n",
       "      <td>1125696.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  edge_id  from_node_id  to_node_id     miles      capacity   volume  \\\n",
       "0  65011    65012         26325       16624  2.391027  45396.405255  44631.0   \n",
       "1  57386    57387         23196         183  3.527136  52814.692258  52543.0   \n",
       "2  41214    41215         16624        7854  1.124715  50570.979389  46904.0   \n",
       "\n",
       "      sum_BC variable_type  variable_value variable_name  nx_edge_id  \\\n",
       "0  5276624.0          Edge       90.718474   Edge_173008     65012.0   \n",
       "1  4671108.0          Edge       90.718474   Edge_152421     57387.0   \n",
       "2  4404746.0          Edge       90.718474   Edge_109126     41215.0   \n",
       "\n",
       "   mode_oid  converted_capacity  converted_volume  \n",
       "0   12925.0        1.089514e+06         1071144.0  \n",
       "1   12949.0        1.267553e+06         1261032.0  \n",
       "2   12525.0        1.213704e+06         1125696.0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create ranked lists of edges to remove.\n",
    "# First, keep only edges in the optimal solution.\n",
    "# Then rank by sum_BC. Then just keep the columns we need, and reset the index.\n",
    "use_cols = ['edge_id', 'from_node_id', 'to_node_id', 'miles', 'capacity', 'volume', 'sum_BC',\n",
    "           'variable_type', 'variable_value', 'variable_name', 'nx_edge_id', 'mode_oid', 'converted_capacity', 'converted_volume']\n",
    "\n",
    "edges_remove = merge_opt[merge_opt['variable_value'] > 0].sort_values(by = 'sum_BC', ascending = False).filter(items = use_cols).reset_index()\n",
    "\n",
    "edges_remove.to_csv(os.path.join(scen_path, 'Edges_to_Remove.csv'),\n",
    "                   index = False)\n",
    "\n",
    "edges_remove.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Scenarios, Disrupt, Run FTOT\n",
    "\n",
    "Create disrupted network by copying everyhing in `scen_path` to a new directory\n",
    "\n",
    "Then overwrites the `networkx_edges` tables in that main.db, with the disrupted versions.\n",
    "\n",
    "##### Assuptions:\n",
    "\n",
    "  1. ArcGIS with 64-bit geoprocessing is installed\n",
    "  2. The FTOT version being used has been modified according to the `README` in this directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 50 scenarios based on qs7_rmp_proc_dest_multi_inputs\n"
     ]
    }
   ],
   "source": [
    "disrupt_type = 'BC' # Can disrupt basaed on betweeness centrality or volume, 'V'\n",
    "disrupt_steps = 50  # This is the number of steps to use. Recommend at least 25.\n",
    "\n",
    "resiliency_disruptions.make_disruption_scenarios(disrupt_type, disrupt_steps, scen_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disrupted 50 scenarios\n"
     ]
    }
   ],
   "source": [
    "resiliency_disruptions.disrupt_network(disrupt_type, disrupt_steps, scen_path, edges_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\FTOT\\program\\ftot.py\n"
     ]
    }
   ],
   "source": [
    "PYTHON = r\"c:\\PYTHON27\\ArcGISx6410.6\\python.exe\"\n",
    "repo_location = %pwd\n",
    "repo_location = os.path.split(repo_location)[0] \n",
    "FTOT = r\"C:\\FTOT\\program\\ftot.py\" # Optionally: os.path.join(repo_location, 'program', 'ftot.py')\n",
    "print(FTOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running o1 for disrupt01\n",
      "Running o2 for disrupt01\n",
      "Running p for disrupt01\n",
      "Running d for disrupt01\n",
      "Preparing to search over o2_log_2020_12_15_10-24-14.log\n",
      "  disrupt_step unmet_demand unmet_cost nedge total_cost\n",
      "0           01            0          0   263      3,146\n",
      "Running o1 for disrupt02\n",
      "Running o2 for disrupt02\n",
      "Running p for disrupt02\n",
      "Running d for disrupt02\n",
      "Preparing to search over o2_log_2020_12_15_10-26-23.log\n",
      "  disrupt_step unmet_demand unmet_cost nedge total_cost\n",
      "0           02            0          0   263      3,146\n",
      "Running o1 for disrupt03\n",
      "Running o2 for disrupt03\n",
      "Running p for disrupt03\n",
      "Running d for disrupt03\n",
      "Preparing to search over o2_log_2020_12_15_10-28-31.log\n",
      "  disrupt_step unmet_demand unmet_cost nedge total_cost\n",
      "0           03            0          0   263      3,146\n",
      "Running o1 for disrupt04\n",
      "Running o2 for disrupt04\n",
      "Running p for disrupt04\n",
      "Running d for disrupt04\n",
      "Preparing to search over o2_log_2020_12_15_10-30-39.log\n",
      "  disrupt_step unmet_demand unmet_cost nedge total_cost\n",
      "0           04            0          0   263      3,146\n",
      "Running o1 for disrupt05\n",
      "Running o2 for disrupt05\n",
      "Running p for disrupt05\n",
      "Running d for disrupt05\n",
      "Preparing to search over o2_log_2020_12_15_10-32-47.log\n",
      "  disrupt_step unmet_demand unmet_cost nedge total_cost\n",
      "0           05            0          0   263      3,146\n",
      "Running o1 for disrupt06\n",
      "Running o2 for disrupt06\n",
      "Running p for disrupt06\n",
      "Running d for disrupt06\n",
      "Preparing to search over o2_log_2020_12_15_10-35-05.log\n",
      "  disrupt_step unmet_demand unmet_cost nedge total_cost\n",
      "0           06            0          0   263      3,146\n",
      "Running o1 for disrupt07\n",
      "Running o2 for disrupt07\n",
      "Running p for disrupt07\n",
      "Running d for disrupt07\n",
      "Preparing to search over o2_log_2020_12_15_10-37-26.log\n",
      "  disrupt_step unmet_demand unmet_cost nedge total_cost\n",
      "0           07            0          0   214      3,158\n",
      "Running o1 for disrupt08\n",
      "Running o2 for disrupt08\n",
      "Running p for disrupt08\n",
      "Running d for disrupt08\n",
      "Preparing to search over o2_log_2020_12_15_10-39-40.log\n",
      "  disrupt_step unmet_demand unmet_cost nedge total_cost\n",
      "0           08            0          0   214      3,158\n",
      "Running o1 for disrupt09\n",
      "Running o2 for disrupt09\n",
      "Running p for disrupt09\n",
      "Running d for disrupt09\n",
      "Preparing to search over o2_log_2020_12_15_10-41-53.log\n",
      "  disrupt_step unmet_demand unmet_cost nedge total_cost\n",
      "0           09            0          0   214      3,158\n",
      "Running o1 for disrupt10\n",
      "Running o2 for disrupt10\n",
      "Running p for disrupt10\n",
      "Running d for disrupt10\n",
      "Preparing to search over o2_log_2020_12_15_10-44-08.log\n",
      "  disrupt_step unmet_demand unmet_cost nedge total_cost\n",
      "0           10            0          0   214      3,158\n",
      "Running o1 for disrupt11\n",
      "Running o2 for disrupt11\n",
      "Running p for disrupt11\n",
      "Running d for disrupt11\n",
      "Preparing to search over o2_log_2020_12_15_10-46-28.log\n",
      "  disrupt_step unmet_demand unmet_cost nedge total_cost\n",
      "0           11            0          0   214      3,158\n",
      "Running o1 for disrupt12\n",
      "Running o2 for disrupt12\n",
      "Running p for disrupt12\n",
      "Running d for disrupt12\n",
      "Preparing to search over o2_log_2020_12_15_10-48-45.log\n",
      "  disrupt_step unmet_demand unmet_cost nedge total_cost\n",
      "0           12            0          0   214      3,158\n",
      "Running o1 for disrupt13\n",
      "Running o2 for disrupt13\n",
      "Running p for disrupt13\n",
      "Running d for disrupt13\n",
      "Preparing to search over o2_log_2020_12_15_10-50-58.log\n",
      "  disrupt_step unmet_demand unmet_cost nedge total_cost\n",
      "0           13            0          0   214      3,158\n",
      "Running o1 for disrupt14\n",
      "Running o2 for disrupt14\n",
      "Running p for disrupt14\n",
      "Running d for disrupt14\n",
      "Preparing to search over o2_log_2020_12_15_10-53-11.log\n",
      "  disrupt_step unmet_demand unmet_cost nedge total_cost\n",
      "0           14            0          0   214      3,158\n",
      "Running o1 for disrupt15\n",
      "Running o2 for disrupt15\n",
      "Running p for disrupt15\n",
      "Running d for disrupt15\n",
      "Preparing to search over o2_log_2020_12_15_10-55-25.log\n",
      "  disrupt_step unmet_demand unmet_cost nedge total_cost\n",
      "0           15            0          0   214      3,158\n",
      "Running o1 for disrupt16\n",
      "Running o2 for disrupt16\n",
      "Running p for disrupt16\n",
      "Running d for disrupt16\n",
      "Preparing to search over o2_log_2020_12_15_10-57-39.log\n",
      "  disrupt_step unmet_demand unmet_cost nedge total_cost\n",
      "0           16            0          0   214      3,160\n",
      "Running o1 for disrupt17\n",
      "Running o2 for disrupt17\n",
      "Running p for disrupt17\n",
      "Running d for disrupt17\n",
      "Preparing to search over o2_log_2020_12_15_10-59-46.log\n",
      "  disrupt_step unmet_demand unmet_cost nedge total_cost\n",
      "0           17            0          0   214      3,160\n",
      "Running o1 for disrupt18\n",
      "Running o2 for disrupt18\n",
      "Running p for disrupt18\n",
      "Running d for disrupt18\n",
      "Preparing to search over o2_log_2020_12_15_11-01-53.log\n",
      "  disrupt_step unmet_demand unmet_cost nedge total_cost\n",
      "0           18            0          0   214      3,160\n",
      "Running o1 for disrupt19\n",
      "Running o2 for disrupt19\n",
      "Running p for disrupt19\n",
      "Running d for disrupt19\n",
      "Preparing to search over o2_log_2020_12_15_11-04-00.log\n",
      "  disrupt_step unmet_demand unmet_cost nedge total_cost\n",
      "0           19            0          0   214      3,160\n",
      "Running o1 for disrupt20\n",
      "Running o2 for disrupt20\n",
      "Running p for disrupt20\n",
      "Running d for disrupt20\n",
      "Preparing to search over o2_log_2020_12_15_11-06-05.log\n",
      "  disrupt_step unmet_demand unmet_cost nedge total_cost\n",
      "0           20            0          0   214      3,160\n",
      "Running o1 for disrupt21\n",
      "Running o2 for disrupt21\n",
      "Running p for disrupt21\n",
      "Running d for disrupt21\n",
      "Preparing to search over o2_log_2020_12_15_11-08-11.log\n",
      "  disrupt_step unmet_demand unmet_cost nedge total_cost\n",
      "0           21            0          0   214      3,160\n",
      "Running o1 for disrupt22\n",
      "Running o2 for disrupt22\n",
      "Running p for disrupt22\n",
      "Running d for disrupt22\n",
      "Preparing to search over o2_log_2020_12_15_11-10-21.log\n",
      "  disrupt_step unmet_demand unmet_cost nedge total_cost\n",
      "0           22            0          0   214      3,160\n",
      "Running o1 for disrupt23\n",
      "Running o2 for disrupt23\n",
      "Running p for disrupt23\n",
      "Running d for disrupt23\n",
      "Preparing to search over o2_log_2020_12_15_11-12-29.log\n",
      "  disrupt_step unmet_demand unmet_cost nedge total_cost\n",
      "0           23            0          0   214      3,160\n",
      "Running o1 for disrupt24\n",
      "Running o2 for disrupt24\n",
      "Running p for disrupt24\n",
      "Running d for disrupt24\n",
      "Preparing to search over o2_log_2020_12_15_11-14-34.log\n",
      "  disrupt_step unmet_demand unmet_cost nedge total_cost\n",
      "0           24            0          0   214      3,160\n",
      "Running o1 for disrupt25\n",
      "Running o2 for disrupt25\n",
      "Running p for disrupt25\n",
      "Running d for disrupt25\n",
      "Preparing to search over o2_log_2020_12_15_11-16-43.log\n",
      "  disrupt_step unmet_demand unmet_cost nedge total_cost\n",
      "0           25            0          0   217      3,161\n",
      "Running o1 for disrupt26\n",
      "Running o2 for disrupt26\n",
      "Running p for disrupt26\n",
      "Running d for disrupt26\n",
      "Preparing to search over o2_log_2020_12_15_11-18-51.log\n",
      "  disrupt_step unmet_demand unmet_cost nedge total_cost\n",
      "0           26            0          0   217      3,161\n",
      "Running o1 for disrupt27\n",
      "Running o2 for disrupt27\n",
      "Running p for disrupt27\n",
      "Running d for disrupt27\n",
      "Preparing to search over o2_log_2020_12_15_11-21-04.log\n",
      "  disrupt_step unmet_demand unmet_cost nedge total_cost\n",
      "0           27            0          0   228      3,364\n",
      "Running o1 for disrupt28\n",
      "Running o2 for disrupt28\n",
      "Running p for disrupt28\n",
      "Running d for disrupt28\n",
      "Preparing to search over o2_log_2020_12_15_11-24-30.log\n",
      "  disrupt_step unmet_demand unmet_cost nedge total_cost\n",
      "0           28            0          0   228      3,364\n",
      "Running o1 for disrupt29\n",
      "Running o2 for disrupt29\n",
      "Running p for disrupt29\n",
      "Running d for disrupt29\n"
     ]
    }
   ],
   "source": [
    "# Begin running O steps of FTOT on the disupted scenarios\n",
    "# This may take several hours, depending on size of the network and number of steps\n",
    "\n",
    "results = resiliency_disruptions.run_o_steps(disrupt_type, disrupt_steps, scen_path, PYTHON, FTOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional: Repeat with volume-based disruptions\n",
    "\n",
    "Creates a separate directory tree for the volume-based disruptions, and carries out the disruption steps on that set.\n",
    "\n",
    "Set the variable `DO_VOLUME` to `True` to run the following steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DO_VOLUME = False\n",
    "\n",
    "if DO_VOLUME:\n",
    "\n",
    "    disrupt_type = 'V'\n",
    "    disrupt_steps = 50\n",
    "\n",
    "    resiliency_disruptions.make_disruption_scenarios(disrupt_type, disrupt_steps, scen_path)\n",
    "    resiliency_disruptions.disrupt_network(disrupt_type, disrupt_steps, scen_path, edges_remove)\n",
    "    results = resiliency_disruptions.run_o_steps(disrupt_type, disrupt_steps, scen_path, PYTHON, FTOT)\n",
    "    results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next: Plot disruption results\n",
    "\n",
    "Run `Plot_Disruption_Results.Rmd` for report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:FTOTenv] *",
   "language": "python",
   "name": "conda-env-FTOTenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
